### Monolingual non-English BERT model to detect and classify abusive language tweets (Multi-class classification


#### Monolingual non-English BERT model for Text Classification
Monolingual non-English BERT refers to a pre-trained language model using the BERT architecture, but trained only on a single language other than English. This type of model is designed to perform natural language processing tasks for a specific non-English language, such as question answering, sentiment analysis, and text classification. The model has been trained on large amounts of text data in the target language, allowing it to capture the nuances and characteristics of the language.

### Dataset
The GermEval 2019 shared task on German tweet analysis was a NLP competition focused on sentiment analysis and irony detection in German tweets. By fine-tuning a pre-trained BERT model on the GermEval 2019 dataset, participants could leverage the model's pre-trained knowledge of the German language to achieve strong results on the competition's tasks.

Lets see how to detect and classify abusive language tweets using this dataset:
